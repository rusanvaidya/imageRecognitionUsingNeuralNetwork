{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zljvTnmqvzn2eGSB56v48HfarfqFFhQl","timestamp":1710271013773}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Perceptron from scratch\n","\n","In this assignment, we will be reimplementing a Neural Networks from scratch.\n","\n","In part A, we are going to build a simple Perceptron on a small dataset that contains only 3 features.\n","\n","<img src='https://drive.google.com/uc?id=1aUtXFBMKUumwfZ-2jmR5SIvNYPaD-t2x' width=\"500\" height=\"250\">\n","\n","Some of the code have already been defined for you. You need only to add your code in the sections specified (marked with **TODO**). Some assert statements have been added to verify the expected outputs are correct. If it does throw an error, this means your implementation is behaving as expected.\n","\n","Note: You are only allowed to use Numpy and Pandas packages for the implemention of Perceptron. You can not packages such as Sklearn or Tensorflow."],"metadata":{"id":"-kcF6GRPRK8Z"}},{"cell_type":"markdown","source":["# 1. Import Required Packages\n","\n","[1.1] We are going to use numpy and random packages"],"metadata":{"id":"wUg1PkpnZAya"}},{"cell_type":"code","source":["import numpy as np\n","import random"],"metadata":{"id":"2rit905Vv-4-","executionInfo":{"status":"ok","timestamp":1710276067406,"user_tz":-660,"elapsed":1378,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# 2. Define Dataset\n","\n","[2.1] We are going to use a simple dataset containing 3 features and 7 observations. The target variable is a binary outcome (either 0 or 1)"],"metadata":{"id":"oFznxXvATvMo"}},{"cell_type":"code","source":["input_set = np.array([[0,1,0], [0,0,1], [1,0,0], [1,1,0], [1,1,1], [0,1,1], [0,1,0]])\n","labels = np.array([[1], [0], [0], [1], [1], [0], [1]])"],"metadata":{"id":"CPN-_r8FLCgS","executionInfo":{"status":"ok","timestamp":1710276068791,"user_tz":-660,"elapsed":20,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# 3. Set Initial Parameters"],"metadata":{"id":"SyYmv5E0T3XP"}},{"cell_type":"markdown","source":["[3.1] Let's set the seed in order to have reproducible outcomes"],"metadata":{"id":"W9kp1UWFUc9N"}},{"cell_type":"code","source":["np.random.seed(42)"],"metadata":{"id":"RgolHUMVT8GA","executionInfo":{"status":"ok","timestamp":1710276068792,"user_tz":-660,"elapsed":20,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["[3.2] **TODO**: Define a function that will create a Numpy array of a given shape with random values.\n","\n","\n","For example, `initialise_array(3,1)` will return an array of dimensions (3,1)that can look like this (values may be different):\n","\n","\n","`array([[0.37454012],\n","       [0.95071431],\n","       [0.73199394]])`"],"metadata":{"id":"5BFSPtVAUpf-"}},{"cell_type":"code","source":["# TODO (Students need to fill this section)\n","def initialise_array(shape):\n","  arr = np.random.rand(*shape)\n","  return arr"],"metadata":{"id":"p_dorRcqVYCT","executionInfo":{"status":"ok","timestamp":1710276068792,"user_tz":-660,"elapsed":18,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["[3.3] **TODO**: Create a Numpy array of shape (3,1) called `init_weights` filled with random values using `initialise_array()` and print them."],"metadata":{"id":"tRQW2pyTXHL7"}},{"cell_type":"code","source":["# TODO (Students need to fill this section)\n","init_weights = initialise_array((3,1))\n","print(init_weights)"],"metadata":{"id":"SKg1QtPdXlQW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710276068793,"user_tz":-660,"elapsed":18,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"1d0bcbaa-268f-47a7-9691-58d5f166ad55"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.37454012]\n"," [0.95071431]\n"," [0.73199394]]\n"]}]},{"cell_type":"markdown","source":["[3.4] **TODO**: Create a Numpy array of shape (1,) called `init_bias` filled with a random value using `initialise_array()` and print it."],"metadata":{"id":"yhl-s-nNXzWL"}},{"cell_type":"code","source":["# TODO (Students need to fill this section)\n","init_bias = initialise_array((1,))\n","print(init_bias)"],"metadata":{"id":"7GWLGBDDX6Ge","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710276068794,"user_tz":-660,"elapsed":14,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"ff83ef0e-4bb6-4e22-d90b-abfedd30a654"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.59865848]\n"]}]},{"cell_type":"markdown","source":["[3.5] Assert statements to check your created variables have the expected shapes"],"metadata":{"id":"-o3y_gmFX9U3"}},{"cell_type":"code","source":["assert init_weights.shape == (3, 1)\n","assert init_bias.shape == (1,)"],"metadata":{"id":"6ZKdef3yWpXh","executionInfo":{"status":"ok","timestamp":1710276073924,"user_tz":-660,"elapsed":424,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["# 4. Define Linear Function\n","In this section we are going to implement the linear function of a neuron:\n","\n","<img src='https://drive.google.com/uc?id=1vhfpGffqletFDzMIvWkCMR2jrHE5MBy5' width=\"500\" height=\"300\">"],"metadata":{"id":"PcWYiiMWYRET"}},{"cell_type":"markdown","source":["[4.1] **TODO**: Define a function that will perform a dot product on the provided X and weights and add the bias to it"],"metadata":{"id":"6LX0Yn_OYw3V"}},{"cell_type":"code","source":["# TODO (Students need to fill this section)\n","def linear(X, weights, bias):\n","  dot_prod = np.dot(X, weights) + bias\n","  return dot_prod"],"metadata":{"id":"ZKx_OQKnZ2UH","executionInfo":{"status":"ok","timestamp":1710276202909,"user_tz":-660,"elapsed":368,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["[4.2] Assert statements to check your linear function is behaving as expected"],"metadata":{"id":"wIhdbPD8bayw"}},{"cell_type":"code","source":["test_weights = [[0.37454012],[0.95071431],[0.73199394]]\n","test_bias = [0.59865848]\n","assert linear(X=input_set[0], weights=test_weights, bias=test_bias)[0] == 1.54937279\n","assert linear(X=input_set[1], weights=test_weights, bias=test_bias)[0] == 1.3306524199999998\n","assert linear(X=input_set[2], weights=test_weights, bias=test_bias)[0] == 0.9731985999999999\n","assert linear(X=input_set[3], weights=test_weights, bias=test_bias)[0] == 1.9239129099999999\n","assert linear(X=input_set[4], weights=test_weights, bias=test_bias)[0] == 2.65590685\n","assert linear(X=input_set[5], weights=test_weights, bias=test_bias)[0] == 2.28136673\n","assert linear(X=input_set[6], weights=test_weights, bias=test_bias)[0] == 1.54937279"],"metadata":{"id":"BF7DDft0aOPU","executionInfo":{"status":"ok","timestamp":1710276207278,"user_tz":-660,"elapsed":345,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["# 5. Activation Function\n","\n","In the forward pass, an activation function is applied on the result of the linear function. We are going to implement the sigmoid function and its derivative:\n","\n","<img src='https://drive.google.com/uc?id=1LK7yjCp4KBICYNvTXzILQUzQbkm7G9xC' width=\"200\" height=\"100\">\n","<img src='https://drive.google.com/uc?id=1f5jUyw0wgiVufNqveeJVZnQc6pOrDJXD' width=\"300\" height=\"100\">\n"],"metadata":{"id":"aPU5Rq62blmE"}},{"cell_type":"markdown","source":["[5.1] **TODO**: Define a function that will implement the sigmoid function"],"metadata":{"id":"rYZHAb-RdNck"}},{"cell_type":"code","source":["# TODO (Students need to fill this section)\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))"],"metadata":{"id":"fmqOuw4afvrH","executionInfo":{"status":"ok","timestamp":1710276382887,"user_tz":-660,"elapsed":390,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["  [5.2] Assert statements to check your sigmoid function is behaving as expected"],"metadata":{"id":"AsWa4glVf4zB"}},{"cell_type":"code","source":["assert sigmoid(0) == 0.5\n","assert sigmoid(1) == 0.7310585786300049\n","assert sigmoid(-1) == 0.2689414213699951\n","assert sigmoid(9999999999999) == 1.0\n","assert sigmoid(-9999999999999) == 0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_36rZRrLfP9","executionInfo":{"status":"ok","timestamp":1710276384528,"user_tz":-660,"elapsed":381,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"989a8694-33f4-4fee-878d-15941d379589"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-41-750209487dd1>:4: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-x))\n"]}]},{"cell_type":"markdown","source":["[5.3] **TODO**: Define a function that will implement the derivative of the sigmoid function"],"metadata":{"id":"MEnLWtDCgWLF"}},{"cell_type":"code","source":["# TODO (Students need to fill this section)\n","def sigmoid_derivative(x):\n","    return sigmoid(x) * (1 - sigmoid(x))"],"metadata":{"id":"OG3SorjugZyS","executionInfo":{"status":"ok","timestamp":1710276490510,"user_tz":-660,"elapsed":411,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["[5.2] Assert statements to check your sigmoid_derivative function is behaving as expected"],"metadata":{"id":"vwVtASkEgeok"}},{"cell_type":"code","source":["assert sigmoid_derivative(0) == 0.25\n","assert sigmoid_derivative(1) == 0.19661193324148185\n","assert sigmoid_derivative(-1) == 0.19661193324148185\n","assert sigmoid_derivative(9999999999999) == 0.0\n","assert sigmoid_derivative(-9999999999999) == 0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVXCcUTZLUpj","executionInfo":{"status":"ok","timestamp":1710276491103,"user_tz":-660,"elapsed":7,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"499bb4ea-c698-49cd-a241-45dbdd0fbeea"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-41-750209487dd1>:4: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-x))\n"]}]},{"cell_type":"markdown","source":["# 6. Forward Pass\n","\n","Now we have everything we need to implement the forward propagation"],"metadata":{"id":"LMacN5l4gkim"}},{"cell_type":"markdown","source":["[6.1] **TODO**: Define a function that will implement the forward pass (apply linear function on the input followed by the sigmoid activation function)"],"metadata":{"id":"ticTCz4Yg1Ze"}},{"cell_type":"code","source":["# TODO (Students need to fill this section)\n","def forward(X, weights, bias):\n","  e = (np.dot(X, weights) + bias)\n","  return sigmoid(e)"],"metadata":{"id":"FuyjHgpahKD9","executionInfo":{"status":"ok","timestamp":1710276949452,"user_tz":-660,"elapsed":372,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["[6.2] Assert statements to check your forward function is behaving as expected"],"metadata":{"id":"l4ZI4yoDhPrX"}},{"cell_type":"code","source":["assert forward(X=input_set[0], weights=test_weights, bias=test_bias)[0] == 0.8248231247647452\n","assert forward(X=input_set[1], weights=test_weights, bias=test_bias)[0] == 0.7909485322272701\n","assert forward(X=input_set[2], weights=test_weights, bias=test_bias)[0] == 0.7257565873271445\n","assert forward(X=input_set[3], weights=test_weights, bias=test_bias)[0] == 0.8725741389540382\n","assert forward(X=input_set[4], weights=test_weights, bias=test_bias)[0] == 0.9343741240208852\n","assert forward(X=input_set[5], weights=test_weights, bias=test_bias)[0] == 0.9073220375080315\n","assert forward(X=input_set[6], weights=test_weights, bias=test_bias)[0] == 0.8248231247647452"],"metadata":{"id":"ebJmLZQtNJMQ","executionInfo":{"status":"ok","timestamp":1710276951635,"user_tz":-660,"elapsed":436,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["# 7. Calculate Error\n","\n","After the forward pass, the Neural Networks will calculate the error between its predictions (output of forward pass) and the actual targets."],"metadata":{"id":"hLlcne6nhTiF"}},{"cell_type":"markdown","source":["[7.1] **TODO**: Define a function that will implement the error calculation (difference between predictions and actual targets)"],"metadata":{"id":"ucesRV6mgi5Q"}},{"cell_type":"code","source":["# TODO (Students need to fill this section)\n","def calculate_error(actual, pred):\n","    return np.subtract(pred, actual)"],"metadata":{"id":"08oSjRvmh3_S","executionInfo":{"status":"ok","timestamp":1710277026502,"user_tz":-660,"elapsed":361,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":[")[7.2] Assert statements to check your calculate_error function is behaving as expected"],"metadata":{"id":"X5TWDZdIh_-D"}},{"cell_type":"code","source":["test_actual = np.array([0,0,0,1,1,1])\n","assert calculate_error(actual=test_actual, pred=[0,0,0,1,1,1]).sum() == 0\n","assert calculate_error(actual=test_actual, pred=[0,0,0,1,1,0]).sum() == -1\n","assert calculate_error(actual=test_actual, pred=[0,0,0,0,0,0]).sum() == -3"],"metadata":{"id":"GAWsb4KpOgL4","executionInfo":{"status":"ok","timestamp":1710277027080,"user_tz":-660,"elapsed":6,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["# 8. Calculate Gradients\n","Once the error has been calculated, a Neural Networks will use this information to update its weights accordingly."],"metadata":{"id":"JUulr3gZiUXx"}},{"cell_type":"markdown","source":["[8.1] Let's creata function that calculate the gradients using the sigmoid derivative function and applying the chain rule."],"metadata":{"id":"zGcDakS9imth"}},{"cell_type":"code","source":["def calculate_gradients(pred, error, input):\n","  dpred = sigmoid_derivative(pred)\n","  z_del = error * dpred\n","  gradients = np.dot(input.T, z_del)\n","  return gradients, z_del"],"metadata":{"id":"pvLIqdu9QQBg","executionInfo":{"status":"ok","timestamp":1710277203202,"user_tz":-660,"elapsed":412,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":["# 9. Training\n","\n","Now that we built all the components of a Neural Networks, we can finally train it on our dataset."],"metadata":{"id":"VS4K4qlSi0kp"}},{"cell_type":"markdown","source":["[9.1] Create 2 variables called `weights` and `bias` that will respectively take the value of `init_weights` and `init_bias`"],"metadata":{"id":"BcOC1D6LjKEX"}},{"cell_type":"code","source":["weights = init_weights\n","bias = init_bias"],"metadata":{"id":"ohEe-udeOZR1","executionInfo":{"status":"ok","timestamp":1710277207345,"user_tz":-660,"elapsed":427,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":["[9.2] Create a variable called `lr` that will be used as the learning rate for updating the weights"],"metadata":{"id":"L8_DzvuqjXOe"}},{"cell_type":"code","source":["lr = 0.5"],"metadata":{"id":"X2DFhqF4jJdz","executionInfo":{"status":"ok","timestamp":1710277208409,"user_tz":-660,"elapsed":5,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":["[9.3] Create a variable called `epochs` with the value 10000. This will the number of times the Neural Networks will process the entire dataset and update its weights"],"metadata":{"id":"QYt-FXr2jhNI"}},{"cell_type":"code","source":["epochs = 10000"],"metadata":{"id":"VS8BWdy5jlra","executionInfo":{"status":"ok","timestamp":1710277210060,"user_tz":-660,"elapsed":6,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":["[9.4] Create a for loop that will perform the training of our Neural Networks"],"metadata":{"id":"zrA8T0r0j0SO"}},{"cell_type":"code","source":["for epoch in range(epochs):\n","    inputs = input_set\n","\n","    # Forward Propagation\n","    z = forward(X=inputs, weights=weights, bias=bias)\n","\n","    # Error\n","    error = calculate_error(actual=labels, pred=z)\n","\n","    # Back Propagation\n","    gradients, z_del = calculate_gradients(pred=z, error=error, input=input_set)\n","\n","    # Update parameters\n","    weights = weights - lr * gradients\n","    for num in z_del:\n","        bias = bias - lr * num\n"],"metadata":{"id":"VvACgpjDMPpI","executionInfo":{"status":"ok","timestamp":1710277213072,"user_tz":-660,"elapsed":1319,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["[9.5] **TODO** Print the final values of `weights` and `bias`"],"metadata":{"id":"K9jYXShpkEp7"}},{"cell_type":"code","source":["# TODO (Students need to fill this section)\n","print(weights)\n","print(bias)"],"metadata":{"id":"POA_DyrRkPup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710277220197,"user_tz":-660,"elapsed":361,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"1f56fa8d-5cfb-4fd0-c318-fc333322aec3"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  9.3914502 ]\n"," [ 20.34119467]\n"," [-10.49443836]]\n","[-14.56039845]\n"]}]},{"cell_type":"markdown","source":["# 10. Compare before and after training\n","\n","Let's compare the predictions of our Neural Networks before (using `init_weights` and `init_bias`) and after the training (using `weights` and `bias`)"],"metadata":{"id":"XAtGwsp6iNuw"}},{"cell_type":"markdown","source":["[10.1] Create a function to display the values of a single observation from the dataset (using its index), the error and the actual target and prediction"],"metadata":{"id":"lU5T4iJTkv8j"}},{"cell_type":"code","source":["def compare_pred(weights, bias, index, X, y):\n","    pred = forward(X=X[index], weights=weights, bias=bias)\n","    actual = y[index]\n","    error = calculate_error(actual, pred)\n","    print(f\"{X[index]} - Error {error} - Actual: {actual} - Pred: {pred}\")"],"metadata":{"id":"j4yTdgGcQms5","executionInfo":{"status":"ok","timestamp":1710277224211,"user_tz":-660,"elapsed":348,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}}},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":["[10.2] Compare the results on the first observation (index 0)"],"metadata":{"id":"kEK9e_0ulM-k"}},{"cell_type":"code","source":["compare_pred(weights=init_weights, bias=init_bias, index=0, X=input_set, y=labels)\n","compare_pred(weights=weights, bias=bias, index=0, X=input_set, y=labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3FA6y1QQc-l","executionInfo":{"status":"ok","timestamp":1710277226719,"user_tz":-660,"elapsed":372,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"d5d1253c-cc1b-4a38-ae13-b6b8964b0d12"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0] - Error [-0.17517688] - Actual: [1] - Pred: [0.82482312]\n","[0 1 0] - Error [-0.00307676] - Actual: [1] - Pred: [0.99692324]\n"]}]},{"cell_type":"markdown","source":["[10.3] Compare the results on the second observation (index 1)"],"metadata":{"id":"emEbcf13lyjE"}},{"cell_type":"code","source":["compare_pred(weights=init_weights, bias=init_bias, index=1, X=input_set, y=labels)\n","compare_pred(weights=weights, bias=bias, index=1, X=input_set, y=labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQMJvQ_FlywM","executionInfo":{"status":"ok","timestamp":1710277230437,"user_tz":-660,"elapsed":401,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"d27698aa-514d-4bea-effd-b41c068c997a"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 1] - Error [0.79094853] - Actual: [0] - Pred: [0.79094853]\n","[0 0 1] - Error [1.31468778e-11] - Actual: [0] - Pred: [1.31468778e-11]\n"]}]},{"cell_type":"markdown","source":["[10.4] Compare the results on the third observation (index 2)"],"metadata":{"id":"BUtP4AmWld0f"}},{"cell_type":"code","source":["compare_pred(weights=init_weights, bias=init_bias, index=2, X=input_set, y=labels)\n","compare_pred(weights=weights, bias=bias, index=2, X=input_set, y=labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imU4LVeqQTXg","executionInfo":{"status":"ok","timestamp":1710277234022,"user_tz":-660,"elapsed":346,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"0aa9abdc-c4c5-4ea6-b13d-00ae718675ea"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 0] - Error [0.72575659] - Actual: [0] - Pred: [0.72575659]\n","[1 0 0] - Error [0.00565835] - Actual: [0] - Pred: [0.00565835]\n"]}]},{"cell_type":"markdown","source":["[10.5] Compare the results on the forth observation (index 3)"],"metadata":{"id":"7n7_s2EAl7M2"}},{"cell_type":"code","source":["compare_pred(weights=init_weights, bias=init_bias, index=3, X=input_set, y=labels)\n","compare_pred(weights=weights, bias=bias, index=3, X=input_set, y=labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GFgNdYm0l7TD","executionInfo":{"status":"ok","timestamp":1710277238938,"user_tz":-660,"elapsed":410,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"3e241adb-751e-49a2-da66-a2df33f29fa2"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 0] - Error [-0.12742586] - Actual: [1] - Pred: [0.87257414]\n","[1 1 0] - Error [-2.57499859e-07] - Actual: [1] - Pred: [0.99999974]\n"]}]},{"cell_type":"markdown","source":["[10.6] Compare the results on the fifth observation (index 4)"],"metadata":{"id":"wnr_LygFmAvK"}},{"cell_type":"code","source":["compare_pred(weights=init_weights, bias=init_bias, index=4, X=input_set, y=labels)\n","compare_pred(weights=weights, bias=bias, index=4, X=input_set, y=labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"euONWlvWmA1o","executionInfo":{"status":"ok","timestamp":1710277244065,"user_tz":-660,"elapsed":423,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"16c856e4-2490-42a6-d970-46c7203002aa"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 1] - Error [-0.06562588] - Actual: [1] - Pred: [0.93437412]\n","[1 1 1] - Error [-0.00921369] - Actual: [1] - Pred: [0.99078631]\n"]}]},{"cell_type":"markdown","source":["[10.7] Compare the results on the sixth observation (index 5)"],"metadata":{"id":"exRMYCRKlhI3"}},{"cell_type":"code","source":["compare_pred(weights=init_weights, bias=init_bias, index=5, X=input_set, y=labels)\n","compare_pred(weights=weights, bias=bias, index=5, X=input_set, y=labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e77HEuVWQN_9","executionInfo":{"status":"ok","timestamp":1710277251040,"user_tz":-660,"elapsed":394,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"7475431e-2524-4652-d855-a8bfdfd163c7"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 1] - Error [0.90732204] - Actual: [0] - Pred: [0.90732204]\n","[0 1 1] - Error [0.00889226] - Actual: [0] - Pred: [0.00889226]\n"]}]},{"cell_type":"markdown","source":["[10.8] Compare the results on the sixth observation (index 5)"],"metadata":{"id":"pumbj0jHmLur"}},{"cell_type":"code","source":["compare_pred(weights=init_weights, bias=init_bias, index=6, X=input_set, y=labels)\n","compare_pred(weights=weights, bias=bias, index=6, X=input_set, y=labels)"],"metadata":{"id":"0glVvMi5mL1s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710277254731,"user_tz":-660,"elapsed":443,"user":{"displayName":"Rusan Vaidya","userId":"12549322092930301106"}},"outputId":"a4a0e8ae-5d77-4180-86ce-1b31e43767da"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0] - Error [-0.17517688] - Actual: [1] - Pred: [0.82482312]\n","[0 1 0] - Error [-0.00307676] - Actual: [1] - Pred: [0.99692324]\n"]}]},{"cell_type":"markdown","source":["We can see after 10000 epochs, our Neural Networks is performing extremely well on our dataset. It has found pretty good values for the weights and bias to make accurate prediction."],"metadata":{"id":"TIKnRhySlqga"}},{"cell_type":"markdown","source":["Please submit this notebook into Canvas. Name it following this rule: *assignment1-partA-\\<student_id\\>.ipynb*"],"metadata":{"id":"C7SGwABjmqN7"}}]}